{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgMk7bvOuO4H"
   },
   "source": [
    "# Case Study: SMS Spam Classification\n",
    "**26 Maret 2022, Review NLP_01 & NLP_02** \n",
    "\n",
    "Created by NLP SQUAD Orbit Future Academy<br>\n",
    "\n",
    "Hari ini, kita akan menyelesaikan kasus klasifikasi SMS spam menggunakan skema seperti gambar di bawah ini.\n",
    "\n",
    "![title](https://undangmi.com/wp-content/uploads/2022/03/Screen-Shot-2022-03-26-at-23.14.46.png)\n",
    "\n",
    "Langkah-langkah penyelesaian melitputi:\n",
    "1. Data collection\n",
    "2. Pre-processing\n",
    "   1. Case Folding\n",
    "   2. Filtering\n",
    "   3. Stopword\n",
    "   4. Stemming\n",
    "3. Feature Extraction\n",
    "   1. BoW\n",
    "   2. TF-IDF\n",
    "4. Feature Selection\n",
    "   1. Chi-Square\n",
    "\n",
    "Pada kasus ini, kita menggunakan data set berbahasa indonesia, sehingga kita membutuhkan *Library Sastrawi* untuk menyelesaikan beberapa permasalahan seperti *stemming*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jVGiQ4nr4uK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Untuk mempermudah, simpan setiap objek agar dapat digunakan untuk pemodelan maupun deployment. Gunakan library Pickle\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dZEvPfm0-gP"
   },
   "outputs": [],
   "source": [
    "!pip -q install sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l990x7Ec2J3Y"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cwyw4gq1sLo0"
   },
   "source": [
    "# Data Acquisition\n",
    "\n",
    "Penjelasan Label \n",
    "* 0: SMS normal \n",
    "* 1: SMS fraud atau penipuan \n",
    "* 2: SMS promo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgvivXY-s5zR"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/dataset_sms_spam_v1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYItGX_jtL3b"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_sms_spam_v1.csv')\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmPgkrIltdLj"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ikc4rlDztnZh"
   },
   "outputs": [],
   "source": [
    "print('Total Jumlah SMS:', data.shape[0], 'data\\n')\n",
    "print('terdiri dari (label):')\n",
    "print('-- [0] SMS Normal\\t:', data[data.label == 0].shape[0], 'data')\n",
    "print('-- [1] Fraud / Penipuan\\t:', data[data.label == 1].shape[0], 'data')\n",
    "print('-- [2] Promo\\t\\t:', data[data.label == 2].shape[0], 'data\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYLfyS4ut0ID"
   },
   "outputs": [],
   "source": [
    "height = data['label'].value_counts()\n",
    "labels = ('SMS Normal', 'SMS Fraud / Penipuan', 'SMS Promo')\n",
    "y_pos = np.arange(len(labels))\n",
    "\n",
    "plt.figure(figsize=(7,4), dpi=80)\n",
    "plt.ylim(0,600)\n",
    "plt.title('Distribusi Kategori SMS', fontweight='bold')\n",
    "plt.xlabel('Kategori', fontweight='bold')\n",
    "plt.ylabel('Jumlah', fontweight='bold')\n",
    "plt.bar(y_pos, height, color=['deepskyblue', 'royalblue', 'skyblue'])\n",
    "plt.xticks(y_pos, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xod-gciUsNF4"
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24IOM3key9Pt"
   },
   "source": [
    "## Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cu5YB-9yzEDf"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Buat fungsi untuk langkah case folding\n",
    "def casefolding(text):\n",
    "  text = text.lower()                               # Mengubah teks menjadi lower case\n",
    "  text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # Menghapus URL\n",
    "  text = re.sub(r'[-+]?[0-9]+', '', text)           # Menghapus angka\n",
    "  text = re.sub(r'[^\\w\\s]','', text)                # Menghapus karakter tanda baca\n",
    "  text = text.strip()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krmI4Lmt50nl"
   },
   "outputs": [],
   "source": [
    "raw_sample = data['teks'].iloc[5]\n",
    "case_folding = casefolding(raw_sample)\n",
    "\n",
    "print('Raw data\\t: ', raw_sample)\n",
    "print('Case folding\\t: ', case_folding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMASVtI1DUN9"
   },
   "source": [
    "## Word Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2f8REO6DTqi"
   },
   "outputs": [],
   "source": [
    "# Download corpus singkatan\n",
    "!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/key_norm.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1TDknAEDnoP"
   },
   "outputs": [],
   "source": [
    "key_norm = pd.read_csv('key_norm.csv')\n",
    "\n",
    "def text_normalize(text):\n",
    "  text = ' '.join([key_norm[key_norm['singkat'] == word]['hasil'].values[0] if (key_norm['singkat'] == word).any() else word for word in text.split()])\n",
    "  text = str.lower(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGq9TrHczTyV"
   },
   "source": [
    "## Filtering (Stopword Removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocKJz_jxzc0o"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ind = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAGVokJg2zGE"
   },
   "outputs": [],
   "source": [
    "len(stopwords_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqkW2aV12XlG"
   },
   "outputs": [],
   "source": [
    "# Lihat daftar stopword yang disediakan NLTK\n",
    "stopwords_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjLJSLrT2S6B"
   },
   "outputs": [],
   "source": [
    "# Buat fungsi untuk langkah stopword removal\n",
    "\n",
    "more_stopword = ['tsel', 'gb', 'rb']                    # Tambahkan kata dalam daftar stopword\n",
    "stopwords_ind = stopwords_ind + more_stopword\n",
    "\n",
    "def remove_stop_words(text):\n",
    "  clean_words = []\n",
    "  text = text.split()\n",
    "  for word in text:\n",
    "      if word not in stopwords_ind:\n",
    "          clean_words.append(word)\n",
    "  return \" \".join(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QICnmDs5YEc"
   },
   "outputs": [],
   "source": [
    "raw_sample = data['teks'].iloc[5]\n",
    "case_folding = casefolding(raw_sample)\n",
    "stopword_removal = remove_stop_words(case_folding)\n",
    "\n",
    "print('Raw data\\t\\t: ', raw_sample)\n",
    "print('Case folding\\t\\t: ', case_folding)\n",
    "print('Stopword removal\\t: ', stopword_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZCnsJAF75-8"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkJptlP_7_TB"
   },
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Buat fungsi untuk langkah stemming bahasa Indonesia\n",
    "def stemming(text):\n",
    "  text = stemmer.stem(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1h6bHSBw8LP5"
   },
   "outputs": [],
   "source": [
    "raw_sample = data['teks'].iloc[5]\n",
    "case_folding = casefolding(raw_sample)\n",
    "stopword_removal = remove_stop_words(case_folding)\n",
    "text_stemming = stemming(stopword_removal)\n",
    "\n",
    "print('Raw data\\t\\t: ', raw_sample)\n",
    "print('Case folding\\t\\t: ', case_folding)\n",
    "print('Stopword removal\\t: ', stopword_removal)\n",
    "print('Stemming\\t\\t: ', text_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYxUP7B69HT3"
   },
   "source": [
    "## Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVe1LL7B9QfT"
   },
   "outputs": [],
   "source": [
    "# Buat fungsi untuk menggabungkan seluruh langkah text preprocessing\n",
    "def text_preprocessing_process(text):\n",
    "  text = casefolding(text)\n",
    "  text = text_normalize(text)\n",
    "  text = remove_stop_words(text)\n",
    "  text = stemming(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNKQowwo9sF4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data['clean_teks'] = data['teks'].apply(text_preprocessing_process)\n",
    "\n",
    "# Perhatikan waktu komputasi ketika proses text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIshorBt932a"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeXfrxd8yQAB"
   },
   "outputs": [],
   "source": [
    "# Simpan data yang telah melalui text preprocessing agar kita tidak perlu menjalankan proses tersebut mulai awal (Opsional)\n",
    "data.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ctyIbyEsPFi"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2t7BhFnWsSZe"
   },
   "outputs": [],
   "source": [
    "# Pisahkan kolom feature dan target\n",
    "X = data['clean_teks']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cxu19lBHvcBb"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqkPMObpvcd2"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXHC1m8wu6bn"
   },
   "source": [
    "## Feature Extraction (Bag of Words & N-Gram)\n",
    "Proses mengubah teks menjadi vektor menggunakan metode BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQO-lSHlu3Zr"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# BoW - Unigram\n",
    "vec = CountVectorizer(ngram_range=(1,1))\n",
    "vec.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hb_aMoZkvdfg"
   },
   "outputs": [],
   "source": [
    "# Melihat Jumlah Fitur\n",
    "print(len(vec.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1dyLe_zv4k4"
   },
   "outputs": [],
   "source": [
    "# Melihat fitur-fitur apa saja yang ada di dalam corpus\n",
    "print(vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fGwg6HowMGn"
   },
   "outputs": [],
   "source": [
    "# Melihat matriks jumlah token\n",
    "# Data ini siap untuk dimasukkan dalam proses pemodelan (machine learning)\n",
    "\n",
    "X_unigram = vec.transform(X).toarray()\n",
    "\n",
    "X_unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3z8w1_ub0tdz"
   },
   "outputs": [],
   "source": [
    "data_unigram = pd.DataFrame(X_unigram, columns=vec.get_feature_names_out())\n",
    "data_unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffNHcPiY02cc"
   },
   "outputs": [],
   "source": [
    "with open('bow.pickle', 'wb') as output:\n",
    "  pickle.dump(X_unigram, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8iKtPqjxR3s"
   },
   "source": [
    "## Feature Extraction (TF-IDF & N-Gram)\n",
    "Proses mengubah teks menjadi vector menggunakan metode TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrcmVBXoxNZm"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1,1))\n",
    "tf_idf.fit(X)\n",
    "\n",
    "X_tf_idf = tf_idf.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELw2koCXyCph"
   },
   "outputs": [],
   "source": [
    "# Melihat Jumlah Fitur\n",
    "print(len(tf_idf.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_5Yo-gpyJKN"
   },
   "outputs": [],
   "source": [
    "# Melihat fitur-fitur apa saja yang ada di dalam corpus\n",
    "print(tf_idf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oza2vPQyNYg"
   },
   "outputs": [],
   "source": [
    "# Melihat matriks jumlah token menggunakan TF IDF, lihat perbedaannya dengan metode BoW\n",
    "# Data ini siap untuk dimasukkan dalam proses pemodelan (machine learning)\n",
    "\n",
    "X_tf_idf = tf_idf.transform(X).toarray()\n",
    "\n",
    "X_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWpH5yE11ulr"
   },
   "outputs": [],
   "source": [
    "data_tf_idf = pd.DataFrame(X_tf_idf, columns=tf_idf.get_feature_names_out())\n",
    "data_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWKDFffQ10EJ"
   },
   "outputs": [],
   "source": [
    "with open('tf_idf.pickle', 'wb') as output:\n",
    "  pickle.dump(X_tf_idf, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5t0Aiuuyz-U"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrRkpAiwyzdZ"
   },
   "outputs": [],
   "source": [
    "# Mengubah nilai data tabular tf-idf menjadi array agar dapat dijalankan pada proses seleksi fitur\n",
    "X = np.array(data_tf_idf)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTPpd8ikyo_I"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Select features according to the k highest scores.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "\n",
    "Compute chi-squared stats between each non-negative feature and class.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html\n",
    "'''\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2 \n",
    "\n",
    "# Ten features with highest chi-squared statistics are selected \n",
    "chi2_features = SelectKBest(chi2, k=1000) \n",
    "X_kbest_features = chi2_features.fit_transform(X, y) \n",
    "  \n",
    "# Reduced features \n",
    "print('Original feature number:', X.shape[1]) \n",
    "print('Reduced feature number:', X_kbest_features.shape[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdseJ5DI_4lk"
   },
   "outputs": [],
   "source": [
    "# chi2_features.scores_ adalah nilai chi-square, semakin tinggi nilainya maka semakin baik fiturnya\n",
    "data_chi2 = pd.DataFrame(chi2_features.scores_, columns=['nilai'])\n",
    "data_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC5Q7oSB_8RP"
   },
   "outputs": [],
   "source": [
    "# Menampilkan fitur beserta nilainya\n",
    "feature = tf_idf.get_feature_names_out()\n",
    "data_chi2['fitur'] = feature\n",
    "data_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9kZnH0rAoUK"
   },
   "outputs": [],
   "source": [
    "# Mengurutkan fitur terbaik\n",
    "data_chi2.sort_values(by='nilai', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEzCob-oA7pt"
   },
   "outputs": [],
   "source": [
    "# Menampilkan mask pada feature yang diseleksi\n",
    "# False berarti fitur tidak terpilih dan True berarti fitur terpilih\n",
    "mask = chi2_features.get_support()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyjhdOE1A-hI"
   },
   "outputs": [],
   "source": [
    "# Menampilkan fitur-fitur terpilih berdasarkan mask atau nilai tertinggi yang sudah dikalkulasi pada Chi-Square\n",
    "new_feature = []\n",
    "for bool, f in zip(mask, feature):\n",
    "  if bool:\n",
    "    new_feature.append(f)\n",
    "  selected_feature = new_feature\n",
    "selected_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjsjV7bjBAw6"
   },
   "outputs": [],
   "source": [
    "# Menampilkan fitur-fitur yang sudah diseleksi \n",
    "# Beserta nilai vektornya pada keseluruhan data untuk dijalankan pada proses machine learning\n",
    "\n",
    "# Hanya k fitur yang terpilih sesuai parameter k yang ditentukan sebelumnya\n",
    "\n",
    "data_selected_feature = pd.DataFrame(X_kbest_features, columns=selected_feature)\n",
    "data_selected_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NU0JcpL6PLEX"
   },
   "outputs": [],
   "source": [
    "with open('best_feature.pickle', 'wb') as output:\n",
    "  pickle.dump(X_kbest_features, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RD0xKtir-1Dd"
   },
   "source": [
    "# Modelling (Machine Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSm4HPLg-3O8"
   },
   "outputs": [],
   "source": [
    "# Coming soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mf4_vPvN-45h"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osgLQLWM-6d8"
   },
   "outputs": [],
   "source": [
    "# Coming soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTcK_p_9--Gp"
   },
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZ5Pw5qg-9IF"
   },
   "outputs": [],
   "source": [
    "# Coming soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SZScK1VBxEJ"
   },
   "source": [
    "# WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4MOec2iBzOs"
   },
   "outputs": [],
   "source": [
    "# Import Library WordCloud. WordCloud digunakan untuk melihat secara visual kata-kata yang paling sering muncul.\n",
    "# Import Library cv2 untuk mengolah gambar menjadi masking WordCloud\n",
    "\n",
    "import cv2\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdXgYyVwHxAy"
   },
   "outputs": [],
   "source": [
    "# Download gambar masking\n",
    "!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/img/cloud.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvv2xSrkIOsj"
   },
   "outputs": [],
   "source": [
    "originalImage = cv2.imread('cloud.jpg')\n",
    "grayImage = cv2.cvtColor(originalImage, cv2.COLOR_BGR2GRAY)\n",
    "(thresh, cloud_mask) = cv2.threshold(grayImage, 100, 255, cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_yRrrXIL3RS"
   },
   "outputs": [],
   "source": [
    "# Tampilkan masking\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "cv2_imshow(cloud_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPmGM2KOJdIZ"
   },
   "outputs": [],
   "source": [
    "# WordCloud Label SMS Normal\n",
    "\n",
    "sms_normal = data[data.label == 0]\n",
    "normal_string = []\n",
    "\n",
    "for t in sms_normal.clean_teks:\n",
    "  normal_string.append(t)\n",
    "\n",
    "normal_string = pd.Series(normal_string).str.cat(sep=' ')\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800, margin=10,\n",
    "                      background_color='white', colormap='Dark2',\n",
    "                      max_font_size=200, min_font_size=25,\n",
    "                      mask=cloud_mask, contour_width=10, contour_color='firebrick',\n",
    "                      max_words=100).generate(normal_string)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AgP0d7eKFbc"
   },
   "outputs": [],
   "source": [
    "# WordCloud Label SMS Fraud / Penipuan\n",
    "\n",
    "sms_fraud = data[data.label == 1]\n",
    "fraud_string = []\n",
    "\n",
    "for t in sms_fraud.clean_teks:\n",
    "  fraud_string.append(t)\n",
    "\n",
    "fraud_string = pd.Series(fraud_string).str.cat(sep=' ')\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800, margin=10,\n",
    "                      background_color='white', colormap='Dark2',\n",
    "                      max_font_size=200, min_font_size=25,\n",
    "                      mask=cloud_mask, contour_width=10, contour_color='firebrick',\n",
    "                      max_words=100).generate(fraud_string)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOFLsBZsKc15"
   },
   "outputs": [],
   "source": [
    "# WordCloud Label SMS Promo\n",
    "\n",
    "sms_promo = data[data.label == 2]\n",
    "promo_string = []\n",
    "\n",
    "for t in sms_promo.clean_teks:\n",
    "  promo_string.append(t)\n",
    "\n",
    "promo_string = pd.Series(promo_string).str.cat(sep=' ')\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=1600, height=800, margin=10,\n",
    "                      background_color='white', colormap='Dark2',\n",
    "                      max_font_size=200, min_font_size=25,\n",
    "                      mask=cloud_mask, contour_width=10, contour_color='firebrick',\n",
    "                      max_words=100).generate(promo_string)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w7LeKa8NdTJ"
   },
   "source": [
    "# Student Activity\n",
    "Lakukan proses di atas menggunakan dataset review product https://drive.google.com/file/d/1qn5WXp-H95_FL_Rx5oqvfZaflYdHsnrF/view?usp=sharing\n",
    "\n",
    "Tugas Anda:\n",
    "- Tentukan langkah pre-processing yang tepat untuk dataset di atas.\n",
    "- Gunakan range `n_gram` yang berbeda. Amati apa perbedaannya.\n",
    "- Menurut Anda, apakah `term` yang dihasilkan (`X_kbest_features`) pada feature selection sudah memiliki informasi yang relevan?\n",
    "\n",
    "Setelah dikerjakan, buatlah resume berdasarkan pengalaman Anda dalam melakukan pre-processing dan feature engineering.\n",
    "\n",
    "Kumpulkan tugas Anda pada: https://s.id/tugas-nlp-ofa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKxVxcMyNf43"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "28Maret-Case Study: NLP_01 & NLP_02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
